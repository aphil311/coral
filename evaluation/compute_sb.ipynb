{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"ft-mistral-7b-instruct-v0.2.jsonl\"\n",
    "\n",
    "# Load the JSONL data from the file\n",
    "import json\n",
    "with open(data_path, \"r\") as f:\n",
    "    raw_judge_results = [json.loads(line) for line in f]\n",
    "    \n",
    "def avg(L):\n",
    "    return sum(L) / len(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Model Count: 1\n",
      "sl-cai: 0.2773\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data.sorry_bench'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     39\u001b[39m fig.update_traces(hovertemplate=\u001b[33m\"\u001b[39m\u001b[33mCategory \u001b[39m\u001b[33m%\u001b[39m\u001b[38;5;132;01m{x}\u001b[39;00m\u001b[33m<br>Model (average score) \u001b[39m\u001b[33m%\u001b[39m\u001b[38;5;132;01m{y}\u001b[39;00m\u001b[33m<br>Score \u001b[39m\u001b[33m%\u001b[39m\u001b[38;5;132;01m{z:.2f}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Set text for every tick\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msorry_bench\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmeta_info\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m category_descriptions_shortest\n\u001b[32m     43\u001b[39m fig.update_xaxes(ticktext=[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m#\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory_descriptions_shortest[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m44\u001b[39m)])\n\u001b[32m     44\u001b[39m fig.update_yaxes(ticktext=[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_score_overall[k]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m sorted_models])\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'data.sorry_bench'"
     ]
    }
   ],
   "source": [
    "judge_results = dict()\n",
    "judge_results[\"sl-cai\"] = [[] for i in range(44)]\n",
    "\n",
    "print(f\"Total Model Count: {len(judge_results)}\")\n",
    "\n",
    "for result in raw_judge_results:\n",
    "    category_id = (result[\"question_id\"] - 1) // 10\n",
    "    if result[\"model\"] in judge_results.keys():\n",
    "        judge_results[result[\"model\"]][category_id].append(result[\"score\"])\n",
    "\n",
    "avg_score_overall = dict()\n",
    "judge_avg_scores_per_category = dict()\n",
    "for model in judge_results:\n",
    "    judge_avg_scores_per_category[model] = []\n",
    "    for category in range(len(judge_results[model])):\n",
    "        judge_avg_scores_per_category[model].append(avg(judge_results[model][category]))\n",
    "    \n",
    "    avg_score_overall[model] = avg(judge_avg_scores_per_category[model])\n",
    "    \n",
    "# Sort the model names by their average score\n",
    "sorted_models = sorted(avg_score_overall, key=avg_score_overall.get, reverse=False)\n",
    "for model in sorted_models:\n",
    "    print(f\"{model}: {avg_score_overall[model]:.4f}\")\n",
    "\n",
    "# For each model, plot a heat map of the average scores per category\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.imshow(\n",
    "    [judge_avg_scores_per_category[model] for model in sorted_models],\n",
    "    color_continuous_scale='Sunsetdark',\n",
    "    text_auto=\".1f\",\n",
    ")\n",
    "\n",
    "# We want to show all ticks...\n",
    "fig.update_xaxes(tickvals=list(range(44)))\n",
    "fig.update_yaxes(tickvals=list(range(len(judge_results))))\n",
    "\n",
    "# Set the hover value for every grid cell\n",
    "fig.update_traces(hovertemplate=\"Category %{x}<br>Model (average score) %{y}<br>Score %{z:.2f}\")\n",
    "\n",
    "# Set text for every tick\n",
    "from data.meta_info import category_descriptions_shortest\n",
    "fig.update_xaxes(ticktext=[f\"#{i+1}: {category_descriptions_shortest[i]}\" for i in range(44)])\n",
    "fig.update_yaxes(ticktext=[f\"{k} ({avg_score_overall[k]:.2f})\" for k in sorted_models])\n",
    "\n",
    "# Rotate the x-axis labels\n",
    "fig.update_xaxes(tickangle=60)\n",
    "\n",
    "# Set figure size\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=1200,\n",
    "    height=950,\n",
    ")\n",
    "\n",
    "# Automatically adjust colorbar height\n",
    "fig.update_layout(coloraxis_colorbar=dict(\n",
    "    thicknessmode=\"pixels\", thickness=20,\n",
    "    lenmode=\"pixels\", len=770,\n",
    "))\n",
    "\n",
    "# Set tight margins\n",
    "# fig.update_layout(margin=dict(l=0, r=0, t=4, b=0))\n",
    "fig.update_layout(margin=dict(l=0, r=0, t=0, b=0))\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# save the plot as png\n",
    "fig.write_image(\"misc/benchmark-results.png\", scale=6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama1b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
